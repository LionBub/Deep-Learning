{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Framework\n",
    "By Luke Doughty  \n",
    "Derived from Grokking Deep Learning by Andrew W. Trask. Much of it is directly copied.  \n",
    "This is meant to build intuition on the inner workings of many deep learning frameworks like Pytorch\n",
    "## Why tensors?\n",
    "Tensors are abstract versions of vectors and matrixes. Vector are one dimensional tensors and matrixes are two dimensional tensors.  \n",
    "The inputs, outputs, and operations are all represented with tensors (vector in, matrix transformation/operation, vector out). We set up our code so we can stack tensors on top of another exactly like layers in our neural network. We have specific instructions on how to backpropagate each type of layer, so we can automatically backpropagate and focus our engineering efforts on forward propagation.\n",
    "## Why don't we have to worry about dimensions?\n",
    "We do. We input the dimensions when we create our weights. When we create a layer, we input the number of inputs and outputs to that layer, and the weights are generated in the constructor occordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    def __init__(self, data, \n",
    "                autoGrad = False,  # autoGrad asks if this tensor should do gradient descent. Useful for drop out regularization.\n",
    "                creators = None, \n",
    "                creation_operation = None, \n",
    "                id = None):\n",
    "        self.data = np.array(data) # What is shape is data?\n",
    "        self.creation_operation = creation_operation\n",
    "        self.creators = creators\n",
    "        self.gradient = None\n",
    "        self.autoGrad = autoGrad\n",
    "        self.children = {}\n",
    "\n",
    "        if (id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "\n",
    "        if (creators is not None):\n",
    "            for creator in creators: # creators is a dictionary where the child's id is the key, and the value at that key tells the number of children the creator tensor has with the current tensor's id.\n",
    "                # keeps track of how many children a tensor has.\n",
    "                if (self.id not in creator.children):\n",
    "                    creator.children[self.id] = 1\n",
    "                else: # should never go down this branch. That would mean that more than one children have been made with the same id.\n",
    "                    creator.children[self.id] += 1\n",
    "\n",
    "    # Checks whether a tensor has received the correct number of gradients from each child.\n",
    "    def all_children_gradients_accounted_for(self):\n",
    "        for id, count in self.children.items():\n",
    "            if(count != 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def backpropagate(self, gradient, gradient_origin = None):\n",
    "        if(self.autoGrad):\n",
    "            if (gradient is None):\n",
    "                gradient = Tensor(np.ones_like(self.data)) # so that we don't have to pass a gradient of 1 the first time we call .backpropagate()\n",
    "            if(gradient_origin is not None):\n",
    "                # checks to make sure you can backpropagate or whether youre waiting for a gradient, in which case decrement the counter.\n",
    "                if(self.children[gradient_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[gradient_origin.id] -= 1\n",
    "\n",
    "            if (self.gradient is None):\n",
    "                self.gradient = gradient\n",
    "            else:\n",
    "                # if we're getting passed gradients from multiple children, add them.\n",
    "                self.gradient += gradient\n",
    "\n",
    "            if (self. creators is not None and (self.all_children_gradients_accounted_for() or gradient_origin is None)):\n",
    "                # here is where we actually start backpropagating.\n",
    "                if(self.creation_operation == \"add\"): # TODO: make this either elif or a switch case\n",
    "                    # backpropagate to both parents/creators. \n",
    "                    self.creators[0].backpropagate(gradient, self)\n",
    "                    self.creators[1].backpropagate(gradient, self)\n",
    "                elif(self.creation_operation == \"neg\"):\n",
    "                    self.creators[0].backpropagate(self.gradient.__neg__())\n",
    "                elif(self.creation_operation == \"sub\"):\n",
    "                    gradient_0 = Tensor(self.gradient.data)\n",
    "                    self.creators[0].backpropagate(gradient_0, self)\n",
    "                    gradient_1 = Tensor(self.gradient.__neg__().data)\n",
    "                    self.creators[1].backpropagate(gradient_1, self)\n",
    "                elif(self.creation_operation == \"mul\"):\n",
    "                    gradient_0 = self.gradient * self.creators[1]\n",
    "                    self.creators[0].backpropagate(gradient_0, self)\n",
    "                    gradient_1 = self.gradient * self.creators[0]\n",
    "                    self.creators[1].backpropagate(gradient_1, self)\n",
    "                elif(self.creation_operation == \"dot\"):\n",
    "                    activation = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    activation_gradient = self.gradient.dot(weights.transpose())\n",
    "                    activation.backpropagate(activation_gradient)\n",
    "                    weights_gradient = self.gradient.transpose().dot(activation).transpose()\n",
    "                    weights.backpropagate(weights_gradient)\n",
    "                elif(self.creation_operation == \"transpose\"):\n",
    "                    self.creators[0].backpropagate(self.gradient.transpose())\n",
    "                elif(\"sum\" in self.creation_operation):\n",
    "                    # since the operation is \"sum\" + str(dimension)\n",
    "                    dimension = int(self.creation_operation.split(\"_\")[1])\n",
    "                    data_shape = self.creators[0].data.shape[dimension]\n",
    "                    self.creators[0].backpropagate(self.gradient.expand(dimension, data_shape))\n",
    "                elif(\"expand\" in self.creation_operation):\n",
    "                    dimension = int(self.creation_operation.split(\"_\")[1])\n",
    "                    self.creators[0].backpropagate(self.gradient.sum(dimension))\n",
    "                elif(self.creation_operation == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.gradient.data))\n",
    "                    # passes the child's gradient multiplied by the derivative of sigmoid at the sigmoid activation output (ie 'self') to calculate the parent's gradient.\n",
    "                    self.creators[0].backpropagate(self.gradient * (self * (ones - self)))\n",
    "                elif(self.creation_operation == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.gradient.data))\n",
    "                    # passes the child's gradient multiplied by the derivative of tanh at the tanh activation output (ie 'self') to calculate the parent's gradient.\n",
    "                    self.creators[0].backpropagate(self.gradient * (ones - (self * self)))\n",
    "                elif(self.creation_operation == \"hardTanh\"):\n",
    "                    max_value, min_value = (1, -1)\n",
    "                    self.creators[0].backpropagate(self.gradient * (self > min_value and self < max_value))\n",
    "                elif(self.creation_operation == \"relu\"):\n",
    "                    self.creators[0].backpropagate(self.gradient * (self > 0))\n",
    "\n",
    "\n",
    "    # add together two tensors\n",
    "    def __add__(self, other):\n",
    "        if (self.autoGrad and other.autoGrad):\n",
    "            return Tensor(self.data + other.data, autoGrad = True, creators = [self, other], creation_operation = \"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    # negates the given tensor. Flips the signs.\n",
    "    def __neg__(self):\n",
    "        if (self.autoGrad):\n",
    "            return Tensor(self.data * -1, autoGrad = True, creators = [self], creation_operation = \"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "\n",
    "    # subtract one tensor from another\n",
    "    def __sub__(self, other):\n",
    "        if (self.autoGrad and other.autoGrad):\n",
    "            return Tensor(self.data - other.data, autoGrad = True, creators = [self, other], creation_operation = \"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    # multiply two tensors\n",
    "    def __mul__(self, other):\n",
    "        if (self.autoGrad and other.autoGrad):\n",
    "            return Tensor(self.data * other.data, autoGrad = True, creators = [self, other], creation_operation = \"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "\n",
    "    # collapses a tensor along a given dimension, adding all numbers along that dimension.\n",
    "    def sum(self, dimension):\n",
    "        if (self.autoGrad):\n",
    "            return Tensor(self.data.sum(dimension), autoGrad = True, creators = [self], creation_operation = \"sum_\" + str(dimension))\n",
    "        return Tensor(self.data.sum(dimension))\n",
    "\n",
    "    # expand a tensor along a given dimension, creating copies of the tensor stacked along the given dimension.\n",
    "    def expand(self, dimension, copies):\n",
    "        # transposition_command tells the order of dimensions for the expanded tensor.\n",
    "        transposition_command = list(range(0, len(self.data.shape)))\n",
    "        transposition_command.insert(dimension, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(transposition_command)\n",
    "\n",
    "        if (self.autoGrad):\n",
    "            return Tensor(new_data, autoGrad = True, creators = [self], creation_operation = \"sum_\" + str(dimension))\n",
    "        return Tensor(self.data.sum(dimension))\n",
    "\n",
    "    # transpose the tensor. In a 1d and 2d tensors, this means swapping the rows and columns.\n",
    "    def transpose(self):\n",
    "        if (self.autoGrad):\n",
    "            return Tensor(self.data.transpose(), autoGrad = True, creators= [self], creation_operation= \"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    # dot product of two tensors. Returns a scalar.\n",
    "    def dot(self, other): # book calls this matrix_multiplication\n",
    "        if(self.autoGrad):\n",
    "            return Tensor(self.data.dot(other.data), autoGrad = True, creators = [self, other], creation_operation = \"dot\")\n",
    "        return Tensor(self.data.dot(other.data))\n",
    "    \n",
    "    # Nonlinearity. Squeezes the values to something between 0 and 1.\n",
    "    def sigmoid(self):\n",
    "        if(self.autoGrad):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)), autoGrad=True, creators=[self], creation_operation=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "    \n",
    "    # Nonlinearity. Squeezes the values to something between -1 and 1\n",
    "    def tanh(self):\n",
    "        if(self.autoGrad):\n",
    "            return Tensor(np.tanh(self.data), autoGrad=True, creators=[self], creation_operation=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    # Nonlinearity. Limits the values to be between a given range. All values greater than the max are set to the max, and all values less than the min are set to the min.\n",
    "    def hardTanh(self):\n",
    "        max_value, min_value = (1, -1) # for the sake of making the backpropagation code easier. I could use a similar approach to sum().\n",
    "        if(self.autoGrad):\n",
    "            return Tensor(max_value if self > max_value else min_value if self < min_value else self, autoGrad=True, creators=[self], creation_operation=\"hardTanh\")\n",
    "        return Tensor(max_value if self > max_value else min_value if self < min_value else self)\n",
    "    \n",
    "    # Nonlinearity. Sets all negative values to zero\n",
    "    def relu(self):\n",
    "        if(self.autoGrad):\n",
    "            return Tensor(self * self > 0, autoGrad=True, creators=[self], creation_operation=\"relu\")\n",
    "        return Tensor(self * self > 0)\n",
    "\n",
    "    # produces the vector as a string, but is supposed to not get rid of any information so an object can be recreated from it.\n",
    "    # similar to __str__, but __str__ is meant to be human-friendly.\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    # prints the tensor as a string\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def zero(self):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.gradient.data *= 0\n",
    "    \n",
    "    def step(self, zero=True):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.data -= parameter.gradient.data * self.alpha\n",
    "            if (zero):\n",
    "                parameter.gradient.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58128304]\n",
      "[0.48988149]\n",
      "[0.41375111]\n",
      "[0.34489412]\n",
      "[0.28210124]\n",
      "[0.2254484]\n",
      "[0.17538853]\n",
      "[0.1324231]\n",
      "[0.09682769]\n",
      "[0.06849361]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    " \n",
    "data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autoGrad=True)\n",
    "target = Tensor(np.array([[0], [1], [0], [1]]), autoGrad=True)\n",
    "\n",
    "weights = list()\n",
    "weights.append(Tensor(np.random.rand(2,3), autoGrad=True))\n",
    "weights.append(Tensor(np.random.rand(3,1), autoGrad=True))\n",
    "\n",
    "optimization = SGD(parameters=weights, alpha=0.1)\n",
    "for iteration in range(10):\n",
    "    prediction = data.dot(weights[0]).dot(weights[1]) # I dont think the first dot outputs a scalar. Is matrix multiplication not the same as dot product?\n",
    "    loss = ((prediction - target) * (prediction - target)).sum(0)\n",
    "    loss.backpropagate(Tensor(np.ones_like(loss.data)))\n",
    "    optimization.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        weights = np.random.randn(num_inputs, num_outputs) * np.sqrt(2.0 / num_inputs)\n",
    "        self.weights = Tensor(weights, autoGrad=True)\n",
    "        self.bias = Tensor(np.zeros(num_outputs), autoGrad=True)\n",
    "\n",
    "        self.parameters.append(self.weights)\n",
    "        self.parameters.append(self.bias)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.dot(self.weights) + self.bias.expand(0, len(input.data))\n",
    "    \n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        parameters = list()\n",
    "        for layer in self.layers:\n",
    "            parameters += layer.get_parameters()\n",
    "        return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.33428272]\n",
      "[0.62282083]\n",
      "[0.19680451]\n",
      "[0.08915535]\n",
      "[0.06028456]\n",
      "[0.049625]\n",
      "[0.04329267]\n",
      "[0.03828787]\n",
      "[0.0339512]\n",
      "[0.03010911]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    " \n",
    "data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autoGrad=True)\n",
    "target = Tensor(np.array([[0], [1], [0], [1]]), autoGrad=True)\n",
    "\n",
    "model = Sequential([Linear(2, 3), Linear(3, 1)]) # weights are automatically generated in the constructor for these Linear layers. We input the shape of each layer.\n",
    "\n",
    "optimization = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "for iteration in range(10):\n",
    "    prediction = model.forward(data)\n",
    "    loss = ((prediction - target) * (prediction - target)).sum(0)\n",
    "    loss.backpropagate(Tensor(np.ones_like(loss.data)))\n",
    "    optimization.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredErrorLoss(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, prediction, target):\n",
    "        return ((prediction - target) * (prediction - target)).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.33428272]\n",
      "[0.62282083]\n",
      "[0.19680451]\n",
      "[0.08915535]\n",
      "[0.06028456]\n",
      "[0.049625]\n",
      "[0.04329267]\n",
      "[0.03828787]\n",
      "[0.0339512]\n",
      "[0.03010911]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    " \n",
    "data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autoGrad=True)\n",
    "target = Tensor(np.array([[0], [1], [0], [1]]), autoGrad=True)\n",
    "\n",
    "model = Sequential([Linear(2, 3), Linear(3, 1)]) # weights are automatically generated in the constructor for these Linear layers. We input the shape of each layer.\n",
    "criterion = MeanSquaredErrorLoss()\n",
    "\n",
    "optimization = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "for iteration in range(10):\n",
    "    prediction = model.forward(data)\n",
    "    loss = criterion.forward(prediction, target) # exactly the same. we just use our MSE loss function object.\n",
    "    loss.backpropagate(Tensor(np.ones_like(loss.data))) \n",
    "    optimization.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinearity Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class HardTanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.hardTanh()\n",
    "    \n",
    "class Relu(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06372865]\n",
      "[0.75148144]\n",
      "[0.57384259]\n",
      "[0.39574294]\n",
      "[0.2482279]\n",
      "[0.15515294]\n",
      "[0.10423398]\n",
      "[0.07571169]\n",
      "[0.05837623]\n",
      "[0.04700013]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    " \n",
    "data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autoGrad=True)\n",
    "target = Tensor(np.array([[0], [1], [0], [1]]), autoGrad=True)\n",
    "\n",
    "model = Sequential([Linear(2, 3), Tanh(), Linear(3, 1), Sigmoid()]) # weights are automatically generated in the constructor for these Linear layers. We input the shape of each layer.\n",
    "criterion = MeanSquaredErrorLoss()\n",
    "\n",
    "optimization = SGD(parameters=model.get_parameters(), alpha=1)\n",
    "\n",
    "for iteration in range(10):\n",
    "    prediction = model.forward(data)\n",
    "    loss = criterion.forward(prediction, target) # exactly the same. we just use our MSE loss function object.\n",
    "    loss.backpropagate(Tensor(np.ones_like(loss.data))) \n",
    "    optimization.step()\n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
