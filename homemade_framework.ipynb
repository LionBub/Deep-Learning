{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Framework\n",
    "By Luke Doughty  \n",
    "Derived from Grokking Deep Learning by Andrew W. Trask\n",
    "## Why tensors?\n",
    "Tensors are abstract versions of vectors and matrixes. Vector are one dimensional tensors and matrixes are two dimensional tensors.  \n",
    "The inputs, outputs, and operations are all represented with tensors (vector in, matrix transformation/operation, vector out). We set up our code so we can stack tensors on top of another exactly like layers in our neural network. We have specific instructions on how to backpropagate each type of layer, so we can automatically backpropagate and focus our engineering efforts on forward propagation.\n",
    "## Why don't we have to worry about dimensions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    def __init__(self, data, \n",
    "                autoGrad = False,  # autoGrad asks if this tensor should do gradient descent. Useful for drop out regularization.\n",
    "                creators = None, \n",
    "                creation_operation = None, \n",
    "                id = None):\n",
    "        self.data = np.array(data) # What is shape is data?\n",
    "        self.creation_operation = creation_operation\n",
    "        self.creators = creators\n",
    "        self.gradient = None\n",
    "        self.autoGrad = autoGrad\n",
    "        self.children = {}\n",
    "\n",
    "        if (id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "\n",
    "        if (creators is not None):\n",
    "            for creator in creators: # creators is a dictionary where the child's id is the key, and the value at that key tells the number of children the creator tensor has with the current tensor's id.\n",
    "                # keeps track of how many children a tensor has.\n",
    "                if (self.id not in creator.children):\n",
    "                    creator.children[self.id] = 1\n",
    "                else: # should never go down this branch. That would mean that more than one children have been made with the same id.\n",
    "                    creator.children[self.id] += 1\n",
    "\n",
    "    # Checks whether a tensor has received the correct number of gradients from each child.\n",
    "    def all_children_gradients_accounted_for(self):\n",
    "        for id, count in self.children.items():\n",
    "            if(count != 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def backpropagate(self, gradient, gradient_origin = None):\n",
    "        if(self.autoGrad):\n",
    "            if(gradient_origin is not None):\n",
    "                # checks to make sure you can backpropagate or whether youre waiting for a gradient, in which case decrement the counter.\n",
    "                if(self.children[gradient_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[gradient_origin.id] -= 1\n",
    "\n",
    "            if (self.gradient is None):\n",
    "                self.gradient = gradient\n",
    "            else:\n",
    "                # if we're getting passed gradients from multiple children, add them.\n",
    "                self.gradient += gradient\n",
    "\n",
    "            if (self. creators is not None and (self.all_children_gradients_accounted_for() or gradient_origin is None)):\n",
    "                # here is where we actually start backpropagating.\n",
    "                if(self.creation_operation == \"add\"): # TODO: make this either elif or a switch case\n",
    "                    # backpropagate to both parents/creators. \n",
    "                    self.creators[0].backpropagate(gradient, self)\n",
    "                    self.creators[1].backpropagate(gradient, self)\n",
    "                elif(self.creation_operation == \"neg\"):\n",
    "                    self.creators[0].backpropagate(self.gradient.__neg__())\n",
    "                elif(self.creation_operation == \"sub\"):\n",
    "                    gradient_0 = Tensor(self.gradient.data)\n",
    "                    self.creators[0].backpropagate(gradient_0, self)\n",
    "                    gradient_1 = Tensor(self.gradient.__neg__().data)\n",
    "                    self.creators[1].backpropagate(gradient_1, self)\n",
    "                elif(self.creation_operation == \"mul\"):\n",
    "                    gradient_0 = self.gradient * self.creators[1]\n",
    "                    self.creators[0].backpropagate(gradient_0, self)\n",
    "                    gradient_1 = self.gradient * self.creators[0]\n",
    "                    self.creators[1].backpropagate(gradient_1, self)\n",
    "                elif(self.creation_operation == \"dot\"):\n",
    "                    activation = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    activation_gradient = self.gradient.dot(weights.transpose())\n",
    "                    activation.backpropagate(activation_gradient)\n",
    "                    weights_gradient = self.gradient.transpose.dot(activation).transpose()\n",
    "                    weights.backpropagate(weights_gradient)\n",
    "                elif(self.creation_operation == \"transpose\"):\n",
    "                    self.creators[0].backpropagate(self.gradient.transpose())\n",
    "                elif(\"sum\" in self.creation_operation):\n",
    "                    # since the operation is \"sum\" + str(dimension)\n",
    "                    dimension = int(self.creation_operation.split(\"_\")[1])\n",
    "                    data_shape = self.creators[0].data.shape[dimension]\n",
    "                    self.creators[0].backpropagate(self.gradient.expand(dimension, data_shape))\n",
    "                elif(\"expand\" in self.creation_operation):\n",
    "                    dimension = int(self.creation_operation.split(\"_\")[1])\n",
    "                    self.creators[0].backpropagate(self.gradient.sum(dimension))\n",
    "\n",
    "\n",
    "    # add together two tensors\n",
    "    def __add__(self, other):\n",
    "        if (self.autoGrad and other.autoGrad):\n",
    "            return Tensor(self.data + other.data, autoGrad = True, creators = [self, other], creation_operation = \"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    # negates the given tensor. Flips the signs.\n",
    "    def __neg__(self):\n",
    "        if (self.autoGrad):\n",
    "            return Tensor(self.data * -1, autoGrad = True, creators = [self], creation_operation = \"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "\n",
    "    # subtract one tensor from another\n",
    "    def __sub__(self, other):\n",
    "        if (self.autoGrad and other.autoGrad):\n",
    "            return Tensor(self.data - other.data, autoGrad = True, creators = [self, other], creation_operation = \"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    # multiply two tensors\n",
    "    def __mul__(self, other):\n",
    "        if (self.autoGrad and other.autoGrad):\n",
    "            return Tensor(self.data * other.data, autoGrad = True, creators = [self, other], creation_operation = \"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "\n",
    "    # collapses a tensor along a given dimension, adding all numbers along that dimension.\n",
    "    def sum(self, dimension):\n",
    "        if (self.autoGrad):\n",
    "            return Tensor(self.data.sum(dimension), autoGrad = True, creators = [self], creation_operation = \"sum_\" + str(dimension))\n",
    "        return Tensor(self.data.sum(dimension))\n",
    "\n",
    "    # expand a tensor along a given dimension, creating copies of the tensor stacked along the given dimension.\n",
    "    def expand(self, dimension, copies):\n",
    "        # transposition_command tells the order of dimensions for the expanded tensor.\n",
    "        transposition_command = list(range(0, len(self.data.shape)))\n",
    "        transposition_command.insert(dimension, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(transposition_command)\n",
    "\n",
    "        if (self.autoGrad):\n",
    "            return Tensor(new_data, autoGrad = True, creators = [self], creation_operation = \"sum_\" + str(dimension))\n",
    "        return Tensor(self.data.sum(dimension))\n",
    "\n",
    "    # transpose the tensor. In a 1d and 2d tensors, this means swapping the rows and columns.\n",
    "    def transpose(self):\n",
    "        if (self.autoGrad):\n",
    "            return Tensor(self.data.transpose(), autoGrad = True, creators= [self], creation_operation= \"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    # dot product of two tensors. Returns a scalar.\n",
    "    def dot(self, other): # book calls this matrix_multiplication\n",
    "        if(self.autoGrad):\n",
    "            return Tensor(self.data.dot(other.data), autoGrad = True, creators = [self, other], creation_operation = \"dot\")\n",
    "        return Tensor(self.data.dot(other.data))\n",
    "\n",
    "    # produces the vector as a string, but is supposed to not get rid of any information so an object can be recreated from it.\n",
    "    # similar to __str__, but __str__ is meant to be human-friendly.\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    # prints the tensor as a string\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
